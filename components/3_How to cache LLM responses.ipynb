{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. How to cache LLM responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangChain library provides a caching mechanism for responses from LLMs (Large Language Models) like OpenAI's models. This caching helps you save costs and improve the speed of your application by storing responses to previous queries so that repeated queries don’t make redundant API calls. Here’s a breakdown of the example you provided:\n",
    "\n",
    "Why Use Caching?\n",
    "* Cost Reduction: If you frequently ask the same questions or requests, caching saves you from making repeated API calls, thus reducing costs.\n",
    "\n",
    "* Speed Improvement: Cached responses are returned almost instantly, which speeds up your application since no new API call is made for a cached query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass()\n",
    "# Please manually enter OpenAI Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# To make the caching really obvious, lets use a slower and older model.\n",
    "# Caching supports newer chat models as well.\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", n=2, best_of=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI is initialized with the model gpt-3.5-turbo-instruct.\n",
    "\n",
    "Parameters:\n",
    "* n=2: The model generates two completions.\n",
    "* best_of=2: Chooses the best completion out of the two, effectively picking the best response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 2.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy couldn't the bicycle stand up by itself?\\nBecause it was two-tired.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_core.caches import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* InMemoryCache is a simple in-memory caching mechanism (all data is stored in memory).\n",
    "* set_llm_cache(InMemoryCache()) configures the cache to be used by the LLM during queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The invoke method sends a request to the LLM.\n",
    "* Since this is the first query, it takes longer because the response is not yet in the cache.\n",
    "* The CPU and wall times are reported, showing how long it took to get the response: around 1.11 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy couldn't the bicycle stand up by itself?\\nBecause it was two-tired.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact same request is made again.\n",
    "*This time, because the response was cached, the retrieval is almost instantaneous (in microseconds).\n",
    "*  The cached response is reused, so no new API call is made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Components\n",
    "* set_llm_cache(InMemoryCache()): This is the command that enables caching. It uses the InMemoryCache class, which stores the data in memory.\n",
    "* llm.invoke(\"Tell me a joke\"): This method sends a prompt to the LLM. The response is cached the first time it's called, and subsequent calls use the cached response if the prompt is identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. How to stream responses from an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stream responses from an LLM in LangChain, you can use several methods depending on your scenario:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1 Synchronous Token-by-Token Streaming: \n",
    "\n",
    "The stream method can be used for synchronous streaming, which returns each part of the response as it’s generated. Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "|Spark|ling| water|,| oh| so| clear\n",
      "|Bubbles dancing|,| without| fear|\n",
      "|Refreshing| taste,| a| delight|\n",
      "|Spark|ling| water|,| my| thirst|'s| delight||"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0, max_tokens=512)\n",
    "\n",
    "# Use the stream method for synchronous streaming\n",
    "for chunk in llm.stream(\"Write me a 1 verse song about sparkling water.\"):\n",
    "    print(chunk, end=\"|\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, each chunk is a part of the response, making it easy to see the streaming behavior in real time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2 Asynchronous Token-by-Token Streaming:\n",
    "\n",
    " Use astream for asynchronous streaming, which is suitable when working with an async environment or application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "|Spark|ling| water|,| oh| so| clear\n",
      "|Bubbles dancing|,| without| fear|\n",
      "|Refreshing| taste,| a| pure| delight|\n",
      "|Spark|ling| water|,| my| thirst|'s| delight||"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0, max_tokens=512)\n",
    "\n",
    "# Use the astream method for async streaming\n",
    "async for chunk in llm.astream(\"Write me a 1 verse song about sparkling water.\"):\n",
    "    print(chunk, end=\"|\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3 Asynchronous Event Streaming: \n",
    "\n",
    "astream_events provides more granular control when dealing with complex workflows that involve multiple steps (e.g., agents or chains). It can emit structured events related to the generation process, like the start or end of a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_llm_start', 'run_id': '3829f406-5169-4542-8baa-72f18f0c47c1', 'name': 'OpenAI', 'tags': [], 'metadata': {}, 'data': {'input': 'Write me a 1 verse song about goldfish on the moon'}, 'parent_ids': []}\n",
      "{'event': 'on_llm_stream', 'run_id': '3829f406-5169-4542-8baa-72f18f0c47c1', 'tags': [], 'metadata': {}, 'name': 'OpenAI', 'data': {'chunk': '\\n\\n'}, 'parent_ids': []}\n",
      "{'event': 'on_llm_stream', 'run_id': '3829f406-5169-4542-8baa-72f18f0c47c1', 'tags': [], 'metadata': {}, 'name': 'OpenAI', 'data': {'chunk': 'Sw'}, 'parent_ids': []}\n",
      "{'event': 'on_llm_stream', 'run_id': '3829f406-5169-4542-8baa-72f18f0c47c1', 'tags': [], 'metadata': {}, 'name': 'OpenAI', 'data': {'chunk': 'imming'}, 'parent_ids': []}\n",
      "...Truncated\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0, max_tokens=512)\n",
    "\n",
    "idx = 0\n",
    "\n",
    "# Stream events asynchronously\n",
    "async for event in llm.astream_events(\n",
    "    \"Write me a 1 verse song about goldfish on the moon\", version=\"v1\"\n",
    "):\n",
    "    idx += 1\n",
    "    if idx >= 5:  # Limit the number of events displayed\n",
    "        print(\"...Truncated\")\n",
    "        break\n",
    "    print(event)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Cases:\n",
    "\n",
    "* Sync Streaming: Ideal for simple applications where blocking behavior is acceptable.\n",
    "* Async Streaming: Great for responsive apps, UIs, or web services that need non-blocking behavior.\n",
    "* Async Event Streaming: Useful in complex workflows that require tracking multiple steps, maintaining states, or debugging with detailed event logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
