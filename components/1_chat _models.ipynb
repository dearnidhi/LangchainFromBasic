{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Using a Chat Model in LangChain\n",
    "Here’s a basic example illustrating how to use a chat model (specifically ChatGroq) in LangChain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat models are advanced language models that handle conversational messages and produce message-based outputs. They excel in interactive applications like chatbots, virtual assistants, and other dialog-based interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic Chat Model Setup with Hugging Face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -qU langchain huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "import os\n",
    "\n",
    "# Set up your Hugging Face API key\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \" put your api key\"\n",
    "\n",
    "# Initialize a language model from Hugging Face\n",
    "chat_model = HuggingFaceHub(repo_id=\"microsoft/DialoGPT-medium\", model_kwargs={\"temperature\": 0.7})\n",
    "\n",
    "# Send a message to simulate a conversation\n",
    "response = chat_model(\"Hello! How can I assist you today?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Function and Tool Calling with Chat Models\n",
    "\n",
    "Chat models can perform specific actions or “tools” based on input. This is useful in applications where certain actions, like retrieving weather info or performing calculations, are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import Tool\n",
    "\n",
    "def calculate_square(num):\n",
    "    return num ** 2\n",
    "\n",
    "# Example function to get square and format response\n",
    "def get_square_response(question):\n",
    "    if \"square of\" in question:\n",
    "        # Extract the number from the question (for simplicity, we use '5')\n",
    "        number = 5  # or use regex to extract numbers dynamically from question\n",
    "        result = calculate_square(number)\n",
    "        return f\"The square of {number} is {result}.\"\n",
    "    else:\n",
    "        return \"I'm here to answer your questions!\"\n",
    "\n",
    "# Sample input\n",
    "question = \"What is the square of 5?\"\n",
    "response = get_square_response(question)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Returning Structured Output\n",
    "For applications that require structured responses, like JSON, you can guide the model to return structured data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_prompt = \"Provide the answer in JSON format with fields 'greeting' and 'response'.\"\n",
    "response = chat_model.invoke(f\"{structured_prompt} Hello!\")\n",
    "print(response)  # Expected output: {\"greeting\": \"Hello\", \"response\": \"Hi there!\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Caching Model Responses\n",
    "Caching saves responses for repeated queries, which can reduce latency and costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import LLMResult\n",
    "\n",
    "# Initialize the model\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "# Simple cache dictionary\n",
    "cache = {}\n",
    "\n",
    "# Caching wrapper function\n",
    "def cached_invoke(prompt):\n",
    "    if prompt in cache:\n",
    "        print(\"Retrieving from cache...\")\n",
    "        return cache[prompt]\n",
    "    else:\n",
    "        print(\"Generating new response...\")\n",
    "        response = chat_model.invoke(prompt)\n",
    "        cache[prompt] = response\n",
    "        return response\n",
    "\n",
    "# Testing the cache\n",
    "response_1 = cached_invoke(\"Hello!\")\n",
    "response_2 = cached_invoke(\"Hello!\")  # This should retrieve from cache\n",
    "print(response_1)\n",
    "print(response_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Log Probabilities of Responses\n",
    "Hugging Face models can provide token probabilities, useful for debugging or evaluating response confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\").bind(logprobs=True)\n",
    "\n",
    "msg = llm.invoke((\"human\", \"how are you today\"))\n",
    "\n",
    "msg.response_metadata[\"logprobs\"][\"content\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = 0\n",
    "full = None\n",
    "for chunk in llm.stream((\"human\", \"how are you today\")):\n",
    "    if ct < 5:\n",
    "        full = chunk if full is None else full + chunk\n",
    "        if \"logprobs\" in full.response_metadata:\n",
    "            print(full.response_metadata[\"logprobs\"][\"content\"])\n",
    "    else:\n",
    "        break\n",
    "    ct += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating a Custom Chat Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain==0.0.208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "class MyCustomChatModel:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    def invoke(self, message):\n",
    "        customized_message = message + \" (Customized)\"\n",
    "        inputs = self.tokenizer(customized_message, return_tensors=\"pt\")\n",
    "        #outputs = self.model.generate(**inputs)\n",
    "        outputs = self.model.generate(**inputs, pad_token_id=self.tokenizer.eos_token_id)\n",
    "\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Usage example\n",
    "my_model = MyCustomChatModel(\"microsoft/DialoGPT-medium\")\n",
    "response = my_model.invoke(\"Hello!\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Streaming Responses\n",
    "For real-time applications, you can stream responses incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# Set up your API key\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"your api key\"\n",
    "\n",
    "# Initialize the chat model with streaming enabled\n",
    "chat_model = ChatOpenAI(streaming=True)\n",
    "\n",
    "# Stream response chunks\n",
    "for chunk in chat_model.stream(\"Tell me something interesting!\"):\n",
    "    print(chunk)  # Outputs each response chunk in real-time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
