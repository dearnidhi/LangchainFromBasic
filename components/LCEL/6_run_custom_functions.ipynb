{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Creating a Custom Function as a Runnable Using RunnableLambda\n",
    "Using RunnableLambda lets you explicitly define any function as a Runnable that you can integrate directly within LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Define a custom function\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "# Wrap it in RunnableLambda\n",
    "length_runnable = RunnableLambda(length_function)\n",
    "\n",
    "# Use it in your pipeline\n",
    "result = length_runnable.invoke(\"Hello, LangChain!\")\n",
    "print(result)  # Outputs the length of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 2. Using the @chain Decorator for Custom Functions\n",
    "The @chain decorator is a convenient way to define a function as a Runnable. When applied, it makes the function behave as a Runnable that can be easily invoked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "Here’s how to use @chain to define a custom function that takes a topic, generates a joke about it, and identifies the joke's subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bears\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define prompts\n",
    "prompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")\n",
    "\n",
    "# Define a custom chain function\n",
    "@chain\n",
    "def custom_chain(topic):\n",
    "    # Generate a joke\n",
    "    joke = ChatOpenAI().invoke(prompt1.invoke({\"topic\": topic}))\n",
    "    # Find the subject of the joke\n",
    "    subject = ChatOpenAI().invoke(prompt2.invoke({\"joke\": joke.content}))\n",
    "    return subject.content\n",
    "\n",
    "# Run the custom chain\n",
    "result = custom_chain.invoke(\"bears\")\n",
    "print(result)  # Outputs the subject of the joke about bears\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Automatic Coercion of Custom Functions in Chains\n",
    "LangChain supports automatic coercion, so you can use a lambda or a standard function directly in a chain without wrapping it. LangChain will treat it as a Runnable.\n",
    "\n",
    "Example:\n",
    "Here’s a chain\n",
    " that takes a topic, generates a story about it, and retrieves the first five characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once \n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me a story about {topic}\")\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Automatically coerces lambda into a Runnable\n",
    "chain = prompt | model | (lambda x: x.content[:5])\n",
    "\n",
    "result = chain.invoke({\"topic\": \"y\"})\n",
    "print(result)  # Outputs the first five characters of the story\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Handling Metadata and Streaming in Custom Functions\n",
    "If your function needs to handle streaming or metadata, LangChain provides ways to structure that using RunnableConfig and RunnableGenerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
    "# Define a function that accepts metadata\n",
    "def custom_metadata_function(text, config: RunnableConfig):\n",
    "    # Process the text here with access to config if needed\n",
    "    return text.upper()\n",
    "\n",
    "# Wrap it with RunnableLambda\n",
    "metadata_runnable = RunnableLambda(custom_metadata_function)\n",
    "\n",
    "# Invoke with metadata\n",
    "result = metadata_runnable.invoke(\"hello\", {\"tags\": [\"example-tag\"]})\n",
    "print(result)  # Outputs: HELLO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Using RunnableGenerator for Streaming\n",
    "If you need a streaming output, use RunnableGenerator to process and yield chunks. This is useful for handling large outputs or data that needs to be processed as it arrives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "a\n",
      "streamed\n",
      "response\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableGenerator\n",
    "\n",
    "# Define the generator function to handle iterators or single strings\n",
    "def custom_streaming_function(input_text):\n",
    "    # Join the input if it's an iterator or iterable, else use as is\n",
    "    if isinstance(input_text, (list, tuple)) or hasattr(input_text, '__iter__'):\n",
    "        input_text = ''.join(input_text)\n",
    "    for word in input_text.split():\n",
    "        yield word\n",
    "\n",
    "# Initialize the RunnableGenerator with the generator function\n",
    "stream_runnable = RunnableGenerator(custom_streaming_function)\n",
    "\n",
    "# Pass a single string to the stream method directly\n",
    "for chunk in stream_runnable.stream(\"This is a streamed response\"):\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
