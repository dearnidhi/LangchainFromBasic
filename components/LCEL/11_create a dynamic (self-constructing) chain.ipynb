{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a dynamic (self-constructing) chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided guide demonstrates how to create a dynamic chain in LangChain that adjusts itself at runtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Concepts:\n",
    "1.Dynamic Chains with RunnableLambda:\n",
    "\n",
    "A RunnableLambda can return another Runnable. When this happens, the new Runnable is executed as part of the overall chain.\n",
    "\n",
    "This allows constructing parts of the chain dynamically based on the input.\n",
    "\n",
    "2.Core Components:\n",
    "\n",
    "RunnablePassthrough: Passes inputs as-is without modification.\n",
    "\n",
    "chain Decorator: Converts a function into a Runnable.\n",
    "\n",
    "Custom Runnables: Define logic to decide which chain or logic path to use at runtime.\n",
    "\n",
    "3. Contextualization Logic:\n",
    "\n",
    "If the chat_history exists, the contextualize_question chain is executed to reformat the question in context.\n",
    "\n",
    "Otherwise, the question is passed directly downstream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000297C8BFC250>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/langchain-anthropic/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000297C8BFC580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/langchain-anthropic/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000297C8BFC8B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/langchain-anthropic/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000297C8BFCA60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/langchain-anthropic/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000297C8BFCC10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/langchain-anthropic/\n",
      "ERROR: Could not find a version that satisfies the requirement langchain-anthropic (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for langchain-anthropic\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Define Prerequisites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Define Components\n",
    "1. Prompts for Contextualization and QA:\n",
    "\n",
    "* contextualize_prompt: Reformats the question in context.\n",
    "* qa_prompt: Generates answers based on retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "contextualize_instructions = \"\"\"Convert the latest user question into a standalone question given the chat history. Don't answer the question, return the question and nothing else (no descriptive text).\"\"\"\n",
    "contextualize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_instructions),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "contextualize_question = contextualize_prompt | llm | StrOutputParser()\n",
    "\n",
    "qa_instructions = (\n",
    "    \"\"\"Answer the user question given the following context:\\n\\n{context}.\"\"\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", qa_instructions), (\"human\", \"{question}\")]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Dynamic Chain Logic (contextualize_if_needed):\n",
    "\n",
    "* Uses RunnableLambda to conditionally return contextualize_question if chat_history is present, or passes the question through directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable, RunnablePassthrough, chain\n",
    "from operator import itemgetter\n",
    "\n",
    "@chain\n",
    "def contextualize_if_needed(input_: dict) -> Runnable:\n",
    "    if input_.get(\"chat_history\"):\n",
    "        return contextualize_question  # Returns Runnable for reformatting\n",
    "    else:\n",
    "        return RunnablePassthrough() | itemgetter(\"question\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 . Mock Retriever (fake_retriever):\n",
    "\n",
    "Simulates document retrieval by returning static context for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def fake_retriever(input_: dict) -> str:\n",
    "    return \"egypt's population in 2024 is about 111 million\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Combine Components into the Full Chain\n",
    " * Dynamic Chain Construction:\n",
    "\n",
    "        * Combines the contextualizer, retriever, and QA steps into a complete processing pipeline.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = (\n",
    "    RunnablePassthrough.assign(question=contextualize_if_needed).assign(\n",
    "        context=fake_retriever\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Invoke and Test\n",
    "Input example with chat_history:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of 2024, Egypt's population is about 111 million.\n"
     ]
    }
   ],
   "source": [
    "response = full_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"what about egypt\",\n",
    "        \"chat_history\": [\n",
    "            (\"human\", \"what's the population of indonesia\"),\n",
    "            (\"ai\", \"about 276 million\"),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
