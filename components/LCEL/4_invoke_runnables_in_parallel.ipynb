{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to invoke runnables in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To invoke multiple runnables in parallel using LangChain, you can use the RunnableParallel primitive. It allows you to execute multiple independent tasks concurrently and return the results in a dictionary format, with each task's result keyed by its name. Here's a breakdown of how to use RunnableParallel in LangChain, along with a full example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Concepts\n",
    "* RunnableParallel: This is a map (dictionary) where each value is a runnable (or something that can be coerced to a runnable, like a function). All tasks will run in parallel and the results will be returned in a dictionary with corresponding keys.\n",
    "\n",
    "* Chaining: You can chain runnables using the | operator to pass the output of one runnable into the next. In RunnableParallel, each entry runs independently, but you can still manipulate and combine results if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Parallelizing Retrieval and Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\10-20-2024\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.033392440527677536, 'start': 0, 'end': 8, 'answer': 'harrison'}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=\"C:/Users/Admin/Desktop/10-20-2024/.env\")\n",
    "\n",
    "# Initialize the vector store with Hugging Face embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\"], embedding=embedding_model\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Initialize Hugging Face QA pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased\")\n",
    "\n",
    "# Retrieve relevant context for the question\n",
    "question = \"where did harrison work?\"\n",
    "retrieved_context = retriever.get_relevant_documents(question)\n",
    "\n",
    "# Prepare the input for the question answering pipeline\n",
    "context = retrieved_context[0].page_content\n",
    "qa_input = {\n",
    "    'context': context,\n",
    "    'question': question\n",
    "}\n",
    "\n",
    "# Directly use Hugging Face QA pipeline for the final answer\n",
    "response = qa_pipeline(qa_input)\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Parallel Task Execution (e.g., Jokes and Poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joke': \"tell me a joke about bear attacks on women. And now we know that women are a part of a problem. You should stand up and say'stop it. This is just more of a problem than this is the right way to do it.'\", 'poem': \"write a 2-line poem about bear and deer together. I'm actually thinking about writing a novel about bear and deer together. I love the idea.\\n\\nI also love writing novels about bear. The fact that so many young people are being\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize Hugging Face pipelines for generating jokes and poems\n",
    "joke_pipeline = pipeline(\"text-generation\", model=\"gpt2\")  # Use GPT-2 for jokes\n",
    "poem_pipeline = pipeline(\"text-generation\", model=\"gpt2\")  # Use GPT-2 for poems\n",
    "\n",
    "# Define chains for joke and poem\n",
    "joke_chain = lambda input_data: joke_pipeline(f\"tell me a joke about {input_data['topic']}\")[0]['generated_text']\n",
    "poem_chain = lambda input_data: poem_pipeline(f\"write a 2-line poem about {input_data['topic']}\")[0]['generated_text']\n",
    "\n",
    "# Create a parallel map of tasks\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "\n",
    "# Invoke the parallel chain\n",
    "response = map_chain.invoke({\"topic\": \"bear\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
