{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to route between sub-chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using a Custom Function (Recommended)\n",
    "# Steps:\n",
    " * Classification Step: Use a chain to classify input into categories (e.g., \"LangChain\", \"Anthropic\", or \"Other\").\n",
    " * Sub-Chains: Define separate sub-chains for handling each category.\n",
    " * Custom Routing Function:\n",
    "      * Define a function that takes input, evaluates conditions, and returns the appropriate sub-chain.\n",
    " * Combine Chains: Use RunnableLambda to integrate the routing logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2912\\3797212930.py:10: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature=0)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2912\\3797212930.py:11: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'This is an answer related to Anthropic'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# Define a prompt template\n",
    "prompt = PromptTemplate(input_variables=[\"question\"], template=\"Answer the following question: {question}\")\n",
    "\n",
    "# Create an LLM chain\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define a routing function\n",
    "def route(inputs):\n",
    "    question = inputs[\"question\"]\n",
    "    if \"Anthropic\" in question:\n",
    "        return {\"answer\": \"This is an answer related to Anthropic\"}\n",
    "    return chain.invoke(inputs)\n",
    "\n",
    "# Combine with a RunnableLambda\n",
    "full_chain = {\n",
    "    \"topic\": chain,  # Replace this if \"topic\" has another purpose\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "} | RunnableLambda(route)\n",
    "\n",
    "# Invoke the chain with input\n",
    "response = full_chain.invoke({\"question\": \"how do I use Anthropic?\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using RunnableBranch (Legacy)\n",
    "# Steps:\n",
    "* Define Branch Conditions: Specify a list of (condition, runnable) pairs.\n",
    "* Default Chain: Include a fallback runnable if no conditions match.\n",
    "* Combine with Classifier: Integrate with the classification chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'Anthropic', 'question': 'What is Anthropic?', 'text': 'Anthropic refers to the principle that the universe and its physical laws are finely tuned to allow for the existence of intelligent life. This concept suggests that the fundamental constants and parameters of the universe are precisely set in a way that allows for the emergence of life as we know it. The anthropic principle has been a topic of debate among scientists and philosophers, with some arguing that it points to a higher purpose or design in the universe, while others see it as a natural consequence of the laws of physics.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Define LLM\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Define chains for specific topics\n",
    "anthropic_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"], template=\"This is a question about Anthropic: {question}\"\n",
    ")\n",
    "anthropic_chain = LLMChain(llm=llm, prompt=anthropic_prompt)\n",
    "\n",
    "langchain_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"], template=\"This is a question about LangChain: {question}\"\n",
    ")\n",
    "langchain_chain = LLMChain(llm=llm, prompt=langchain_prompt)\n",
    "\n",
    "# Define a general fallback chain\n",
    "general_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"], template=\"Answer this question: {question}\"\n",
    ")\n",
    "general_chain = LLMChain(llm=llm, prompt=general_prompt)\n",
    "\n",
    "# Define branch logic\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: \"anthropic\" in x[\"topic\"].lower(), anthropic_chain),\n",
    "    (lambda x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),\n",
    "    general_chain,  # Default chain\n",
    ")\n",
    "\n",
    "# Combine with classifier\n",
    "full_chain = {\"topic\": lambda x: x[\"topic\"], \"question\": lambda x: x[\"question\"]} | branch\n",
    "\n",
    "# Test the chain\n",
    "response = full_chain.invoke({\"topic\": \"Anthropic\", \"question\": \"What is Anthropic?\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "anthropic_chain: Handles inputs related to \"Anthropic\" using a specific prompt.\n",
    "\n",
    "langchain_chain: Handles inputs related to \"LangChain.\"\n",
    "\n",
    "general_chain: Acts as a fallback for other topics.\n",
    "\n",
    "RunnableBranch: Directs inputs to the correct chain based on the topic.\n",
    "\n",
    "Combining Chains: The full_chain ensures the routing logic works seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
