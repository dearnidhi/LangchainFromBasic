{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to stream runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The LangChain Runnable Interface supports both synchronous and asynchronous streaming methods (stream and astream) to make applications feel more responsive by outputting intermediate results as they’re generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Components for Streaming in LangChain\n",
    "* LLMs and Chat Models: Language models are usually the slowest part of LLM applications, so streaming their output token-by-token can improve responsiveness. LangChain's streaming API lets us process each token immediately rather than waiting for the entire output.\n",
    "\n",
    "* Streaming Methods:\n",
    "\n",
    "   a. sync stream: Streams the final output in chunks.\n",
    " \n",
    "   b.  async astream: Asynchronous version, for environments where async processing is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|The| color| of| the| sky| can| vary| depending| on| several| factors|,| including| time| of| day|,| weather| conditions|,| and| atmospheric| composition|.| Generally|,| during| a| clear| day|,| the| sky| appears| blue| due| to| Ray|leigh| scattering|,| where| shorter| blue| wavelengths| of| sunlight| are| scattered| in| all| directions| by| the| gases| and| particles| in| the| atmosphere|.| At| sunrise| and| sunset|,| the| sky| can| display| a| range| of| colors|,| including| red|,| orange|,| and| pink|,| due| to| the| angle| of| the| sun| and| the| increased| distance| the| light| travels| through| the| atmosphere|.| On| cloudy| or| storm|y| days|,| the| sky| can| appear| gray| or| over|cast|.||"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "chunks = []\n",
    "for chunk in model.stream(\"what color is the sky?\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|The| color| of| the| sky| typically| appears| blue| during| the| day| due| to| the| scattering| of| sunlight| by| the| Earth's| atmosphere|.| This| phenomenon|,| known| as| Ray|leigh| scattering|,| causes| shorter| wavelengths| of| light| (|blue| and| violet|)| to| scatter| more| than| longer| wavelengths| (|red|,| orange|,| yellow|).| However|,| the| sky| can| also| appear| in| various| shades|,| such| as| gray| during| over|cast| weather|,| orange| or| pink| during| sunrise| and| sunset|,| and| even| black| at| night| when| the| sun| is| below| the| horizon|.||"
     ]
    }
   ],
   "source": [
    "## Asynchronous streaming\n",
    "\n",
    "chunks = []\n",
    "async for chunk in model.astream(\"what color is the sky?\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-db79cf26-58b0-48b2-93e5-fcd34e8697e7')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains with LangChain Expression Language (LCEL)\n",
    "Using LangChain’s Expression Language, you can construct chains of components (e.g., prompt templates, models, parsers) that can stream the output seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Why| did| the| par|rot| wear| a| rain|coat|?\n",
      "\n",
      "|Because| it| wanted| to| be| a| poly|uns|aturated|!||"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "async for chunk in chain.astream({\"topic\": \"parrot\"}):\n",
    "    print(chunk, end=\"|\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling JSON Streaming\n",
    "To stream JSON output while generating it incrementally:\n",
    "\n",
    "Use JsonOutputParser to operate on partial JSON.\n",
    "\n",
    "Use generator functions (yield) to handle partial data and operate on streams incrementally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This import brings in JsonOutputParser, a component used to parse the output of language models into JSON. This is especially useful for structured tasks, ensuring the model's response conforms to the JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model: This represents a language model, like OpenAI GPT or Hugging Face models, which generates the text output.\n",
    "\n",
    "* JsonOutputParser(): Acts as a parser in the chain, ensuring that the raw text generated by the model is parsed into valid JSON.\n",
    "\n",
    "* | (Pipe Operator): Connects components in the LangChain framework, indicating that the output of the model is fed into the JsonOutputParser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'countries': []}\n",
      "{'countries': [{}]}\n",
      "{'countries': [{'name': ''}]}\n",
      "{'countries': [{'name': 'France'}]}\n",
      "{'countries': [{'name': 'France', 'population': 652}]}\n",
      "{'countries': [{'name': 'France', 'population': 652735}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain'}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 467}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 467547}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 46754778}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 46754778}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 46754778}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan'}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': 126}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': 126476}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': 126476461}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "chain = (\n",
    "    model | JsonOutputParser()\n",
    ")  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n",
    "async for text in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\"\n",
    "):\n",
    "    print(text, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* chain.astream: This is the asynchronous method to start the chain, generating and processing the model's output.\n",
    "* Prompt: The input prompt is a clear instruction for the model to generate a specific JSON structure.\n",
    "* async for: Used for consuming asynchronous iterable results. The chain streams the output incrementally, processing it in chunks rather than waiting for the entire response to complete.\n",
    "* print(text, flush=True): Prints each chunk of streamed output, ensuring it's displayed immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Non-streaming Components\n",
    "Some components, like Retrievers, don’t natively support streaming. For these cases:\n",
    "\n",
    "Place the non-streaming components early in the chain.\n",
    "\n",
    "Streaming starts after the last non-streaming step, maintaining partial streaming from there.\n",
    "\n",
    "LangChain’s streaming approach, along with LCEL chains, allows for flexibility in handling complex LLM applications that feel responsive to users by delivering intermediate outputs rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={}, page_content='harrison worked at kensho'),\n",
       "  Document(metadata={}, page_content='harrison likes spicy food')]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\", \"harrison likes spicy food\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "chunks = [chunk for chunk in retriever.stream(\"where did harrison work?\")]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from langchain) (3.10.10)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from langchain) (0.3.21)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from langchain) (0.1.136)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from langchain) (2.10.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.15.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.9)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (3.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\admin\\desktop\\10-20-2024\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = (\n",
    "    {\n",
    "        \"context\": retriever.with_config(run_name=\"Docs\"),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|H|arrison| worked| at| Kens|ho|.| Kens|ho| is| known| for| its| innovative| approach| to| data| analysis|,| leveraging| advanced| algorithms| to| provide| insights| in| real| time|.| The| atmosphere| in| the| office| is| vibrant| and| collaborative|,| with| team| members| often| brainstorming| ideas| over| lunch|.| Additionally|,| Kens|ho| hosts| regular| workshops| to| keep| employees| updated| on| the| latest| trends| in| technology| and| data| science|.||"
     ]
    }
   ],
   "source": [
    "for chunk in retrieval_chain.stream(\n",
    "    \"Where did harrison work? \" \"Write 3 made up sentences about this place.\"\n",
    "):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
