{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to pass through arguments from one step to the next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Why Passing Data Matters in Chains\n",
    "\n",
    "In LangChain, we often need to send data through different steps in a sequence, known as a chain. Sometimes, you might want certain data to continue unchanged through each step. For this, RunnablePassthrough comes into play. When combined with RunnableParallel, it becomes easy to run multiple processes at once while keeping data intact for certain steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Runnable Classes Overview\n",
    "RunnablePassthrough: This class passes data through as-is. It’s ideal when you want to retain the original data in a specific key and use it later.\n",
    "\n",
    "RunnableParallel: This class enables multiple tasks to execute in parallel, such as processing data in one step while passing other data through unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Example: Basic Usage of RunnablePassthrough and RunnableParallel\n",
    "Here’s a simple example where we have two keys, passed and modified. passed uses RunnablePassthrough to keep the original input unchanged, while modified uses a function to increment a number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'passed': {'num': 1}, 'modified': 2}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),   # Keeps original data unchanged\n",
    "    modified=lambda x: x[\"num\"] + 1 # Increments the number in \"num\"\n",
    ")\n",
    "\n",
    "result = runnable.invoke({\"num\": 1})\n",
    "print(result) # Output: {'passed': {'num': 1}, 'modified': 2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passed retains {'num': 1}, showing how RunnablePassthrough keeps data intact.\n",
    "\n",
    "modified increments num by 1, resulting in modified: 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Real-World Example: Using RunnablePassthrough with a Prompt\n",
    "\n",
    "In the second example, we set up a retrieval chain that searches a vector store for context. Here’s how it works:\n",
    "\n",
    "Vector Store and Retriever: We use FAISS, a vector store, to store and retrieve context.\n",
    "\n",
    "Prompt and Model: We create a template prompt that requires context and question keys and pass the prompt to a model to generate an answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harrison worked at Kensho.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts([\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "answer = retrieval_chain.invoke(\"where did harrison work?\")\n",
    "print(answer) # Output: \"Harrison worked at Kensho.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Retriever fetches relevant information from FAISS, providing context.\n",
    "\n",
    "RunnablePassthrough passes the user’s question (\"where did harrison work?\") directly to the question key.\n",
    "\n",
    "Prompt and Model use the context and question to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
