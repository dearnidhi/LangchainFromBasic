{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to add fallbacks to a runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding fallbacks to a Runnable in LangChain is a way to ensure robustness in your system when dealing with unpredictable API behavior or other runtime errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Why Use Fallbacks?\n",
    "Error Handling: Manage rate limits, downtime, or other errors from an LLM API.\n",
    "\n",
    "Dynamic Adjustments: Use different prompts or models when switching between LLMs.\n",
    "\n",
    "Performance Optimization: Start with a faster/cheaper model and fallback to a better-performing one when necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Steps to Implement Fallbacks\n",
    "Install Required Packages\n",
    "\n",
    "Ensure you have the required LangChain packages installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain langchain-openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Fallback for LLM API Errors\n",
    "If an error occurs while invoking one model, you can define a fallback to another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to invoke OpenAI...\n",
      "Caught RateLimitError: Rate limit\n",
      "Triggering fallback model due to rate limit\n"
     ]
    }
   ],
   "source": [
    "from unittest.mock import patch\n",
    "import httpx\n",
    "from openai import RateLimitError\n",
    "from langchain_openai import ChatOpenAI\n",
    "#from langchain_anthropic import ChatAnthropic  # Make sure to install this if it's available\n",
    "\n",
    "# Initialize OpenAI model (with no retries to avoid retrying on rate limits)\n",
    "openai_llm = ChatOpenAI(model=\"gpt-4\", max_retries=0)\n",
    "\n",
    "# Initialize fallback model (using a different model for fallback)\n",
    "#anthropic_llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "\n",
    "# Create a model chain with fallback\n",
    "llm_with_fallbacks = openai_llm.with_fallbacks([openai_llm])\n",
    "\n",
    "# Mock a RateLimitError for OpenAI\n",
    "error = RateLimitError(\"Rate limit\", response=httpx.Response(429, request=httpx.Request(\"GET\", \"/\")), body=\"Rate limit exceeded\")\n",
    "\n",
    "\n",
    "\n",
    "# Use patch to simulate the API failure (i.e., OpenAI returns rate-limited error)\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    \n",
    "    try:\n",
    "        print(\"Trying to invoke OpenAI...\")\n",
    "        # Try invoking the LLM with fallbacks\n",
    "        result = llm_with_fallbacks.invoke(\"Why did the chicken cross the road?\")\n",
    "        print(f\"Result from fallback model: {result}\")\n",
    "    except RateLimitError as e:\n",
    "        print(\"Caught RateLimitError:\", str(e))\n",
    "        print(\"Triggering fallback model due to rate limit\")\n",
    "    except Exception as e:\n",
    "        print(f\"Caught unexpected error: {str(e)}\")\n",
    "        # You can define other fallback models or mechanisms here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to invoke OpenAI...\n",
      "Result from fallback model: content='To get to the other side.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 15, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-17b80de7-3c93-4c4e-853d-ddcb654e879d-0' usage_metadata={'input_tokens': 15, 'output_tokens': 7, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Trying to invoke OpenAI...\")\n",
    "    # Try invoking the LLM with fallbacks\n",
    "    result = llm_with_fallbacks.invoke(\"Why did the chicken cross the road?\")\n",
    "    print(f\"Result from fallback model: {result}\")\n",
    "except RateLimitError as e:\n",
    "    print(\"Caught RateLimitError:\", str(e))\n",
    "    print(\"Triggering fallback model due to rate limit\")\n",
    "except Exception as e:\n",
    "    print(f\"Caught unexpected error: {str(e)}\")\n",
    "    # You can define other fallback models or mechanisms here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the fallback model (ChatAnthropic) responded correctly with the expected result: \"To get to the other side.\"\n",
    "\n",
    "Here's a breakdown of the output:\n",
    "\n",
    "* Content: \"To get to the other side\" (the answer to the joke)\n",
    "* Metadata: It includes token usage details, such as:\n",
    "\n",
    "  completion_tokens: 7\n",
    "\n",
    "  prompt_tokens: 15\n",
    "\n",
    "  total_tokens: 22\n",
    "  \n",
    "* Model Info: The fallback model used is gpt-4-0613"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fallback for Entire Chains\n",
    "Fallbacks are also applicable for complex chains, where each LLM may require its own tailored prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Response: To show off its slow-but-steady determination and gracefulness.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "\n",
    "# Chain 1: Chat Model\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're a nice assistant.\"),\n",
    "    (\"human\", \"Why did the {animal} cross the road?\")\n",
    "])\n",
    "chat_model = ChatOpenAI(model=\"gpt-fake\")  # Simulate a failure\n",
    "chat_chain = chat_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "# Chain 2: Regular Model\n",
    "prompt_template = \"\"\"Instructions: Include a compliment in your response.\n",
    "\n",
    "Question: Why did the {animal} cross the road?\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "regular_model = OpenAI()\n",
    "regular_chain = prompt | regular_model\n",
    "\n",
    "# Combine Chains with Fallbacks\n",
    "chain_with_fallbacks = chat_chain.with_fallbacks([regular_chain])\n",
    "response = chain_with_fallbacks.invoke({\"animal\": \"turtle\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fallback for Context Window Issues\n",
    "Handle cases where the input length exceeds the context window of one model by falling back to a model with a larger context window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f how to handle context window issues and fallback mechanisms properly in LangChain, where the model will automatically switch to a larger context window model if the input exceeds the limit.\n",
    "\n",
    "In this example, we use ChatOpenAI models with two different context sizes (gpt-3.5-turbo and gpt-3.5-turbo-16k), with the latter being used as a fallback when the input exceeds the context window of the shorter model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9236\\4088969087.py:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  short_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # Regular context window\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: content='4000' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 15009, 'total_tokens': 15011, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-22d158e4-9c61-435a-a294-981073cb27a0-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize two models with different context window sizes\n",
    "short_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # Regular context window\n",
    "long_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")  # Larger context window\n",
    "\n",
    "# Create a fallback chain: if short_llm fails, use long_llm\n",
    "llm_with_fallbacks = short_llm.with_fallbacks([long_llm])\n",
    "\n",
    "# Example input exceeding the context window\n",
    "inputs = \"What is the next number: \" + \", \".join([str(i) for i in range(1, 4000)])\n",
    "\n",
    "try:\n",
    "    # Invoke the model with fallbacks\n",
    "    response = llm_with_fallbacks.invoke(inputs)\n",
    "    print(\"Response:\", response)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "\n",
    "1. Models Setup: We define two models, one (gpt-3.5-turbo) for regular context windows and another (gpt-3.5-turbo-16k) for larger context windows.\n",
    "\n",
    "2. Fallback Mechanism: The with_fallbacks() method is used to create a fallback chain where, if the shorter model (short_llm) encounters an issue (e.g., exceeding the context window), the fallback model (long_llm) is used.\n",
    "\n",
    "3. Handling Long Inputs: The input string is intentionally long (by generating a sequence of numbers), ensuring that it exceeds the context window of the smaller model. The fallback will trigger to handle this.\n",
    "\n",
    "# Expected Result:\n",
    "* If the prompt exceeds the context window of gpt-3.5-turbo, the fallback model (gpt-3.5-turbo-16k) will be used to process the input.\n",
    "* The output will depend on the model’s handling of the input, and the result will be printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fallback to a Better Model for Specific Tasks\n",
    "Start with a cheaper model and fallback to a more capable one for cases where additional precision or parsing is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to use a fallback system in LangChain to switch to a more suitable model for specific tasks when the primary model is not able to handle the input requirements or limitations. This could be useful for more complex or specialized queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Model Response: content='This research paper delves into the latest trends in AI and machine learning, focusing on deep learning and natural language processing. The paper includes detailed information on methodologies, experimental results, and references to provide insights into the advancements in these areas. It highlights the importance of deep learning and natural language processing in the field of AI and discusses the potential applications and implications of these technologies.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 53, 'total_tokens': 127, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-5cda80b1-50fc-489b-96f5-06c213e18f24-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain  # Corrected import\n",
    "\n",
    "# Define two models: one with a regular context window and another with a larger context window\n",
    "regular_model = ChatOpenAI(model=\"gpt-3.5-turbo\")  # Standard model with a smaller context window\n",
    "extended_model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")  # Model with a larger context window\n",
    "\n",
    "# Create a fallback system where the extended model will be used for long or complex queries\n",
    "llm_with_fallbacks = regular_model.with_fallbacks([extended_model])\n",
    "\n",
    "# Example task that requires a larger context window (e.g., processing a long input or a complex query)\n",
    "task_description = (\n",
    "    \"Summarize this research paper, which contains detailed information about the latest trends in AI and machine learning. \"\n",
    "    \"The paper includes numerous references, methodologies, and experimental results that provide insights into the field of deep learning and natural language processing.\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Attempt to process the task with the regular model\n",
    "    response = llm_with_fallbacks.invoke(task_description)\n",
    "    print(\"Regular Model Response:\", response)\n",
    "except Exception as e:\n",
    "    print(f\"Error with regular model: {e}\")\n",
    "    print(\"Switching to extended model...\")\n",
    "\n",
    "    # If the task exceeds the regular model's capabilities, the fallback model will be used\n",
    "    response = extended_model.invoke(task_description)\n",
    "    print(\"Extended Model Response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "1. Models Setup: We're using two versions of the ChatOpenAI model:\n",
    "* regular_model with a default context window.\n",
    "* extended_model with a larger context window to handle more complex or lengthy tasks.\n",
    "2. Fallback Mechanism: The with_fallbacks() method combines both models so that if the regular_model encounters a limitation (like exceeding its context window or handling a complex query), the extended_model will be used automatically.\n",
    "3. Handling Specific Tasks: When a query or input requires a larger context or is too long for the regular_model, the fallback will switch to the extended_model for better performance.\n",
    "# Expected Result:\n",
    "* If the query or task is within the capabilities of the regular_model, it will handle it without switching.\n",
    "* If the input is too long or complex, the extended_model (with the larger context window) will process the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Best Practices for Using Fallbacks\n",
    "* Disable Automatic Retries: Ensure the initial LLM is not set to retry, as this delays the fallback from being triggered.\n",
    "* Different Prompts for Different Models: Customize the prompts to suit the capabilities of each fallback model.\n",
    "* Monitor Performance: Continuously track fallback usage to understand why errors occur and refine the primary model or chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
