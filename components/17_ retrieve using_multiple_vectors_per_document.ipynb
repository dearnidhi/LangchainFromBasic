{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to retrieve using multiple vectors per document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Why Multiple Vectors Per Document?\n",
    "When we work with large documents, storing only a single vector (embedding) per document may not be enough. Different parts of the document might be relevant to different questions or contexts. So, instead of representing the entire document with one vector, we can split it into smaller chunks and create multiple vectors for each chunk. This helps in:\n",
    "\n",
    "* Precision: Chunks allow embeddings to capture specific meanings.\n",
    "* Recall: Even if a question is specific, it can still link back to a larger document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. How Does It Work?\n",
    "LangChain provides a way to split documents into smaller pieces and associate them with the parent document. These smaller chunks can be embedded (converted into vectors), making it easier to search within a document based on different parts.\n",
    "\n",
    "Here’s a step-by-step guide on the different methods to do this:\n",
    "\n",
    "a) Splitting into Smaller Chunks (ParentDocumentRetriever)\n",
    "\n",
    "* Idea: Split a large document into smaller parts (like splitting a book into paragraphs).\n",
    "* Usage: Embed each smaller chunk separately.\n",
    "* Retrieval: When searching, you retrieve the chunk that matches your query but get the context from the entire parent document.\n",
    "\n",
    "Example:\n",
    "\n",
    "Split a document into 400-character chunks and embed each one.\n",
    "When you search for \"justice breyer\", it matches the relevant chunk but returns the full parent document.\n",
    "\n",
    "b) Summarizing Chunks\n",
    "* Idea: Instead of splitting the document into smaller pieces, create a summary for each document.\n",
    "* Usage: Embed the summary along with the original document. This allows you to capture the gist of the content.\n",
    "* Retrieval: The summary embedding helps in accurate retrieval, but you can also retrieve the full parent document if needed.\n",
    "\n",
    "Example:\n",
    "\n",
    "Summarize each document using a language model.\n",
    "Store and embed the summary. When you search, you first get the relevant summary, then pull the full document.\n",
    "\n",
    "c) Hypothetical Questions\n",
    "* Idea: Generate possible questions that a document could answer.\n",
    "* Usage: Create and embed these hypothetical questions along with the document. This way, even if the query isn't directly in the document, the embedded question can help retrieve it.\n",
    "* Retrieval: Questions provide a wider net for catching semantically similar queries.\n",
    "\n",
    "Example:\n",
    "\n",
    "For a document about a Supreme Court judge, generate questions like \"What impact did Judge X have on legal cases?\"\n",
    "When searching, the hypothetical questions guide the retrieval even if the exact wording doesn't appear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Components Involved\n",
    "Here's how we implement these ideas:\n",
    "\n",
    "* 1. Vector Store:\n",
    "This is where the vectors (embeddings) are stored.\n",
    "For example, a vector store might store vectors of smaller document chunks or summaries.\n",
    "* 2. Document Store:\n",
    "This holds the full, original parent documents and associates them with unique identifiers.\n",
    "It’s like a database that links back to the original source.\n",
    "* 3. MultiVectorRetriever:\n",
    "The retriever acts as a bridge between vector search and the document store.\n",
    "It uses embeddings to search for relevant information and pulls the larger document context if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Example Workflow\n",
    "Here’s a simplified example to visualize the process:\n",
    "\n",
    "* Split a document into smaller chunks (e.g., 400 characters each).\n",
    "* Embed each chunk (convert it into a vector).\n",
    "* Store the embeddings in a vector store.\n",
    "* Store the full document with an identifier in the document store.\n",
    "* When you query the retriever:\n",
    "  * * It finds the best matching chunk using the vector store.\n",
    "  * * Returns the larger parent document for more context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Advanced Use-Cases\n",
    "Summaries can replace smaller chunks if you prefer a concise view.\n",
    "Hypothetical Questions can guide searches when the query doesn’t directly match the document content.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways:\n",
    "* Precision: Smaller chunks = better embedding accuracy.\n",
    "* Recall: Parent document linkage ensures no loss of context.\n",
    "* Flexibility: Summaries and hypothetical questions offer alternatives to exact matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How LangChain Helps:\n",
    "LangChain’s MultiVectorRetriever abstracts much of this complexity. You just need to:\n",
    "\n",
    "* Choose how you want to split the document.\n",
    "* Embed those parts (chunks, summaries, questions).\n",
    "* Retrieve results with better precision and flexibility.\n",
    "\n",
    "This setup ensures your retrieval is both contextually rich (full document context) and semantically accurate (specific parts matching the query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain-chroma langchain langchain-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Small Chunk: Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\n",
      "Retrieved Full Document Length: 9874\n",
      "Retrieved Hypothetical Questions: ['What impact would appointing a highly qualified Supreme Court justice have on the judicial system and its future decisions?', 'How might comprehensive immigration reform, including a path to citizenship for Dreamers, affect the U.S. economy and society?', 'How could the trajectory of Y Combinator have changed if Sam Altman refused the offer to become president?', \"How would the Bipartisan Infrastructure Law impact America's global economic competitiveness?\"]\n",
      "Retrieved Full Document Length: 9194\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import getpass\n",
    "import os\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever, SearchType\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Step 1: Load documents\n",
    "# Load documents from text files using the TextLoader\n",
    "loaders = [\n",
    "    TextLoader(r\"C:\\Users\\Admin\\Desktop\\10-20-2024\\data\\paul_graham_essay.txt\", encoding=\"utf-8\"),\n",
    "    TextLoader(r\"C:\\Users\\Admin\\Desktop\\10-20-2024\\data\\state_of_the_union.txt\", encoding=\"utf-8\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "# Step 2: Split documents into larger chunks\n",
    "# Split the documents into larger chunks of 10,000 characters\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# Step 3: Set up the vector store using OpenAI embeddings\n",
    "# Initialize a Chroma vector store to store embeddings of child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", \n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Step 4: Create smaller chunks (for better semantic capture)\n",
    "# Create smaller chunks (400 characters) from the larger documents\n",
    "store = InMemoryByteStore()  # Storage layer for parent documents\n",
    "id_key = \"doc_id\"  # Unique identifier key for documents\n",
    "retriever = MultiVectorRetriever(  # Initialize the retriever\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# Generate unique IDs for each document\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Split each document into smaller sub-documents and associate them with a parent ID\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "sub_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]\n",
    "    _sub_docs = child_text_splitter.split_documents([doc])\n",
    "    for _doc in _sub_docs:\n",
    "        _doc.metadata[id_key] = _id  # Store the parent document ID in metadata\n",
    "    sub_docs.extend(_sub_docs)\n",
    "\n",
    "# Add the smaller chunks to the vector store and associate them with parent documents\n",
    "retriever.vectorstore.add_documents(sub_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))\n",
    "\n",
    "# Step 5: Test retrieval\n",
    "# Perform similarity search with the vector store to get smaller chunks\n",
    "result = retriever.vectorstore.similarity_search(\"justice breyer\")[0]\n",
    "print(\"Retrieved Small Chunk:\", result.page_content)\n",
    "\n",
    "# Use the retriever to get the larger parent document\n",
    "retrieved_docs = retriever.invoke(\"justice breyer\")\n",
    "print(\"Retrieved Full Document Length:\", len(retrieved_docs[0].page_content))\n",
    "\n",
    "# Step 6: Summary-based retrieval\n",
    "# Use an LLM to summarize the documents and create embeddings based on summaries\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()  # Set OpenAI API key\n",
    "\n",
    "# Initialize the language model (OpenAI Chat model)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Create a chain to summarize documents using the LLM\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}  # Take the document content as input\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | llm  # Use the language model to generate a summary\n",
    "    | StrOutputParser()  # Parse the output to get the summary string\n",
    ")\n",
    "\n",
    "# Generate summaries for all documents\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})\n",
    "\n",
    "# Initialize a new vector store for summaries\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# Set up a new retriever for summaries\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# Create Document objects from summaries and store them in the vector store\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))\n",
    "\n",
    "# Step 7: Hypothetical questions-based retrieval\n",
    "# Generate hypothetical questions for documents using an LLM\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class HypotheticalQuestions(BaseModel):\n",
    "    questions: List[str] = Field(..., description=\"List of questions\")\n",
    "\n",
    "# Create a chain to generate hypothetical questions\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}  # Input is the document content\n",
    "    | ChatPromptTemplate.from_template(\n",
    "        \"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer:\\n\\n{doc}\"\n",
    "    )\n",
    "    | ChatOpenAI(max_retries=0, model=\"gpt-4o\").with_structured_output(\n",
    "        HypotheticalQuestions\n",
    "    )\n",
    "    | (lambda x: x.questions)  # Extract the questions from the output\n",
    ")\n",
    "\n",
    "# Generate hypothetical questions for all documents\n",
    "hypothetical_questions = chain.batch(docs, {\"max_concurrency\": 5})\n",
    "\n",
    "# Set up a vector store for hypothetical questions\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"hypo-questions\", \n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# Create Document objects from hypothetical questions and store them\n",
    "question_docs = []\n",
    "for i, question_list in enumerate(hypothetical_questions):\n",
    "    question_docs.extend(\n",
    "        [Document(page_content=s, metadata={id_key: doc_ids[i]}) for s in question_list]\n",
    "    )\n",
    "retriever.vectorstore.add_documents(question_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))\n",
    "\n",
    "# Perform a similarity search using hypothetical questions\n",
    "sub_docs = retriever.vectorstore.similarity_search(\"justice breyer\")\n",
    "print(\"Retrieved Hypothetical Questions:\", [doc.page_content for doc in sub_docs])\n",
    "\n",
    "# Use the retriever to get the larger source document\n",
    "retrieved_docs = retriever.invoke(\"justice breyer\")\n",
    "print(\"Retrieved Full Document Length:\", len(retrieved_docs[0].page_content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Document Loading and Splitting:\n",
    "\n",
    "     * * The documents are loaded and split into chunks using RecursiveCharacterTextSplitter.\n",
    "     * * Smaller chunks are embedded while larger \"parent\" documents are retained.\n",
    "\n",
    "* Retrieving with Smaller Chunks:\n",
    "\n",
    "     * * Embedding smaller chunks helps capture semantics better.\n",
    "     * * Retrieval returns larger parent documents associated with smaller chunks.\n",
    "* Summary-based Retrieval:\n",
    "\n",
    "     * * Summarize documents using an LLM.\n",
    "     * * Use these summaries to embed and retrieve relevant documents.\n",
    "* Hypothetical Questions-based Retrieval:\n",
    "\n",
    "    * * Generate questions that a document can answer using an LLM.\n",
    "    * * Embed these questions for retrieval, improving retrieval accuracy for specific queries.\n",
    "\n",
    "\n",
    "This code allows for effective multi-vector retrieval, supporting scenarios like retrieving precise answers or more comprehensive document segments.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
