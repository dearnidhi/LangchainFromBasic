{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messages\n",
    " are a crucial component of the interaction between the user and the large language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. HumanMessage\n",
    "A HumanMessage represents a message from the user. It generally consists only of the content property, which is a string.\n",
    "\n",
    "Role: Human\n",
    "\n",
    "Content: Typically a string representing the user's input.\n",
    "\n",
    "Description: Represents a message from the user. It generally consists only of the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Create a HumanMessage with a content string\n",
    "message = HumanMessage(content=\"Hello, how are you?\")\n",
    "\n",
    "# Print the message\n",
    "print(message.content)  # Output: Hello, how are you?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.AIMessage:\n",
    "An AIMessage represents a message from the model. This may have additional kwargs in it, such as tool_calls if using OpenAI tool calling.\n",
    "\n",
    "Role: Model\n",
    "\n",
    "Content: The response generated by the model.\n",
    "\n",
    "Additional kwargs: Can include specific data related to the response.\n",
    "\n",
    "Usage: The response from the Groq model would be encapsulated in this message type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm good, thanks!\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import AIMessage\n",
    "\n",
    "# Create an AIMessage with a content string and additional kwargs\n",
    "message = AIMessage(content=\"I'm good, thanks!\", additional_kwargs={\"tool_calls\": [\"tool1\", \"tool2\"]})\n",
    "\n",
    "# Print the message\n",
    "print(message.content)  # Output: I'm good, thanks!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.SystemMessage\n",
    "\n",
    "A SystemMessage represents a system message, which tells the model how to behave. This generally only consists of the content property.\n",
    "\n",
    "Role: System\n",
    "\n",
    "Content: Instructions for the model's behavior.\n",
    "\n",
    "Usage: Can be used to set up the context or guidelines for the model's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please respond with a greeting.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "\n",
    "# Create a SystemMessage with a content string\n",
    "message = SystemMessage(content=\"Please respond with a greeting.\")\n",
    "\n",
    "# Print the message content\n",
    "print(message.content)  # Output: Please respond with a greeting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. FunctionMessage:\n",
    "A FunctionMessage represents the result of a function call. In addition to the role and content properties, this message has a name parameter which conveys the name of the function that was called to produce this result.\n",
    "\n",
    "Role: Function\n",
    "\n",
    "Content: Result of a function call.\n",
    "\n",
    "Name: The name of the function that produced the result.\n",
    "\n",
    "Usage: Use this when calling specific functions and receiving their outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import FunctionMessage\n",
    "\n",
    "# Create a FunctionMessage with name \"greeting_function\"\n",
    "message = FunctionMessage(name=\"greeting_function\", content=\"Hello, how are you?\")\n",
    "\n",
    "# Print the message content\n",
    "print(message.content)  # Output: Hello, how are you?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ToolMessage\n",
    "Role: Tool\n",
    "\n",
    "Content: Result of a tool call.\n",
    "\n",
    "Tool_call_id: Identifier for the tool call.\n",
    "\n",
    "Usage: Use this when integrating tool calls that produce results, especially when using tools provided by the OpenAI API.\n",
    "\n",
    "A ToolMessage represents the result of a tool call. In addition to the role and content, this message has a tool_call_id parameter which conveys the id of the call to the tool that was called to produce this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the result of the tool call.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "# Create a ToolMessage\n",
    "message = ToolMessage(tool_call_id=\"tool1\", content=\"This is the result of the tool call.\")\n",
    "\n",
    "# Print the message content\n",
    "print(message.content)  # Output: This is the result of the tool call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"According to the latest forecast, today is looking quite pleasant! A high-pressure system is currently dominating the region, bringing clear skies and plenty of sunshine. Temperatures are expected to reach the mid-60s to low 70s Fahrenheit, with a gentle breeze blowing in from the west.\\n\\nAs we head into the afternoon, the winds may pick up slightly, but overall, it's going to be a beautiful day to get outside and enjoy the fresh air! If you're planning any outdoor activities, make sure to pack a light jacket or sweater, as it might get a bit cooler as the sun begins to set.\\n\\nOf course, I'll be keeping a close eye on the forecast, so if any changes arise, I'll be sure to update you accordingly. But for now, it's looking like a fantastic day ahead!\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 167, 'prompt_tokens': 28, 'total_tokens': 195, 'completion_time': 0.139166667, 'prompt_time': 0.003683751, 'queue_time': 0.010473258999999999, 'total_time': 0.142850418}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None} id='run-af298e4d-1d7b-46f3-9681-317f2ffe20df-0' usage_metadata={'input_tokens': 28, 'output_tokens': 167, 'total_tokens': 195}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv  # Ensure you have python-dotenv installed\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Groq model using the API Key from .env\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "\n",
    "# Create a system message to guide the model's behavior\n",
    "system_message = SystemMessage(content=\"You are a weather assistant.\")\n",
    "\n",
    "# Create a human message from the user\n",
    "user_input = \"What's the weather like today?\"\n",
    "human_message = HumanMessage(content=user_input)\n",
    "\n",
    "# Prepare the messages for the model\n",
    "messages = [system_message, human_message]\n",
    "\n",
    "# Get the model's response (assuming response is a string)\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# Print the response directly\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
