{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to reorder retrieved results to mitigate the \"lost in the middle\" effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the issue known as the \"lost in the middle\" effect in Retrieval-Augmented Generation (RAG) applications, a strategy can be applied to reorder documents. This effect happens when models miss important information because it's positioned in the middle of a long list of documents, which can result in a drop in performance, especially when dealing with many retrieved documents (e.g., more than ten).\n",
    "\n",
    "# Here's a simplified explanation of the solution:\n",
    "\n",
    "* Relevance Sorting Issue: \n",
    "\n",
    "Typically, when documents are retrieved from a vector store (like a database of documents), they are ordered by relevance to a queryâ€”most relevant first and least relevant last. However, when these documents are used as input for models like LLMs, there's a risk that information in the middle gets overlooked.\n",
    "\n",
    "* Reordering for Better Context:\n",
    "\n",
    " To help LLMs better capture the most important information, you can rearrange the documents. The idea is to place the most relevant documents at the beginning and the end, while the less relevant documents go in the middle. This way, key information is more likely to be noticed.\n",
    "\n",
    "# Implementation Example:\n",
    "\n",
    "You start by embedding a set of documents using a tool like OpenAI embeddings.\n",
    "These documents are stored in a vector store that allows you to retrieve them by relevance to a query.\n",
    "After retrieval, you use a specific tool called LongContextReorder to rearrange the documents, positioning the most relevant ones at the extremes (beginning and end)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Step-by-Step Code Explanation\n",
    "Embed Documents: You create a set of documents (e.g., some sentences about basketball and the Boston Celtics). These documents are converted into embeddings (numerical vectors) using OpenAI's embedding tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* InMemoryVectorStore: A vector store is a database that stores vectors, which are numerical representations of text. InMemoryVectorStore is a type of vector store that keeps all the stored vectors in memory, making it fast to use for testing or small-scale tasks.\n",
    "\n",
    "* OpenAIEmbeddings: This is a tool that generates embeddings (numerical representations) of text using an OpenAI model. These embeddings allow you to measure how similar different pieces of text are by comparing their vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here, embeddings is an instance of the OpenAIEmbeddings class, which will be used to generate vector representations of the text data.\n",
    "\n",
    "* The texts list contains a collection of sentences. Each sentence will be converted into a numerical vector using OpenAIEmbeddings. These vectors represent the meaning of each sentence in a way that can be compared mathematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* InMemoryVectorStore.from_texts: This line creates an in-memory vector store from the given list of texts. Each text is embedded using OpenAIEmbeddings, resulting in a set of vectors stored in memory.\n",
    "   * * from_texts: A class method that takes a list of texts and an embedding method to create the vector store.\n",
    "   * * embedding=embeddings: Specifies that the embedding model (in this case, OpenAIEmbeddings) should be used to convert the texts into vectors.\n",
    "* .as_retriever(): Converts the vector store into a retriever. A retriever is responsible for finding the most relevant documents (texts) based on a similarity search.\n",
    "  * * search_kwargs={\"k\": 10}: This argument specifies that you want to retrieve the top 10 most relevant documents for a given query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* query: This is the user's question or search input. In this case, the query asks for information about the \"Celtics.\"\n",
    "* retriever.invoke(query): This method takes the query and:\n",
    "    * * Converts the query into an embedding using the same OpenAIEmbeddings method, resulting in a vector that represents the query.\n",
    "    * * Compares the query vector with the vectors of all the stored documents using a similarity metric (like cosine similarity).\n",
    "    * * Returns the top k (in this case, 10) most relevant documents that are closest to the query vector in terms of similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Output (docs)\n",
    "\n",
    "docs will be a list of the top 10 documents from the texts list, sorted by their relevance to the query, \"What can you tell me about the Celtics?\"\n",
    "Each document in docs will likely be accompanied by additional metadata, such as the relevance score or any other details tracked during retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries:\n",
    "# InMemoryVectorStore is a vector store that keeps all vectors in memory.\n",
    "# OpenAIEmbeddings is used to generate vector embeddings of text.\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Initialize the embeddings model using OpenAI.\n",
    "# This model will convert each text into a numerical vector representation.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create a list of texts/documents that we want to store and retrieve.\n",
    "texts = [\n",
    "    \"Basketball is a great sport.\",\n",
    "    # A generic statement about basketball.\n",
    "    \n",
    "    \"Larry Bird was an iconic NBA player.\",\n",
    "    # A specific statement about Larry Bird, a famous NBA player.\n",
    "]\n",
    "\n",
    "# Create an in-memory vector store from the list of texts:\n",
    "# - Converts each text into a vector using OpenAIEmbeddings.\n",
    "# - Stores these vectors in memory for quick access.\n",
    "# - The vector store is then turned into a 'retriever' that can find documents.\n",
    "retriever = InMemoryVectorStore.from_texts(texts, embedding=embeddings).as_retriever(\n",
    "    search_kwargs={\"k\": 10}  # Set to retrieve the top 10 most relevant documents.\n",
    ")\n",
    "\n",
    "# Define the query for the search:\n",
    "# - This is the question or input we're interested in.\n",
    "# - We're asking about \"the Celtics,\" a famous basketball team.\n",
    "query = \"What can you tell me about the Celtics?\"\n",
    "\n",
    "# Use the retriever to find documents relevant to the query:\n",
    "# - The query is also converted to a vector using OpenAIEmbeddings.\n",
    "# - The retriever compares the query vector with the stored vectors.\n",
    "# - It retrieves the top 10 documents that are most similar (relevant) to the query.\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "# At this point, 'docs' will contain a list of documents from the 'texts' list,\n",
    "# sorted by their relevance to the query about the Celtics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Document Retrieval: \n",
    "The retriever returns documents sorted by relevance to your query (in this case, about the Celtics).\n",
    "\n",
    "Before Reordering: Most relevant documents appear first.\n",
    "\n",
    "# 3 Reorder Documents:\n",
    "\n",
    "Use LongContextReorder to rearrange the documents so that the most relevant documents are at the start and end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LongContextReorder transformer from langchain_community:\n",
    "# This tool helps to reorder retrieved documents to mitigate the \"lost in the middle\" effect.\n",
    "# It moves the most relevant documents to the beginning and end of the context.\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "\n",
    "# Initialize the LongContextReorder transformer.\n",
    "# This will be used to reorder the retrieved documents.\n",
    "reordering = LongContextReorder()\n",
    "\n",
    "# Apply the reordering to the list of retrieved documents (docs):\n",
    "# - The transformer takes the list of documents and reorders them.\n",
    "# - The most relevant documents will be moved to the beginning and end of the list.\n",
    "# - This reordering helps improve the chances of LLMs capturing key information\n",
    "#   because they often focus more on the start and end of the context.\n",
    "reordered_docs = reordering.transform_documents(docs)\n",
    "\n",
    "# After this step, 'reordered_docs' will contain the documents with adjusted order.\n",
    "# More relevant documents will be positioned at the extremes (start and end),\n",
    "# while less relevant ones will be placed in the middle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Reordering:\n",
    " The key documents are now positioned at the extremes (first and last), while less important ones are in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 .Use in a QA Chain:\n",
    "\n",
    "The reordered documents are then passed to a simple question-answering (QA) chain.\n",
    "A prompt template is used to format the documents and query for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Boston Celtics are a professional basketball team based in Boston, Massachusetts. They are one of the most successful teams in NBA history, known for their rich legacy and numerous championships. The Celtics were founded in 1946 and are a part of the Eastern Conference. Larry Bird, mentioned in your texts, is one of the most famous players to have ever played for the Celtics, contributing significantly to their success during the 1980s and helping them win three NBA championships. The team is also recognized for its strong fan base and its rivalry with teams like the Los Angeles Lakers.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary classes for combining documents and handling prompts:\n",
    "# - `create_stuff_documents_chain` is used to create a chain that takes documents and generates a response.\n",
    "# - `PromptTemplate` helps format prompts with custom input.\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the language model (LLM) using the OpenAI GPT-4o-mini model.\n",
    "# This model will handle generating responses based on the provided context.\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Define a template for the prompt:\n",
    "# - This is the format for how the question will be asked to the LLM.\n",
    "# - It includes placeholders `{context}` for the documents' content\n",
    "#   and `{query}` for the user's question.\n",
    "prompt_template = \"\"\"\n",
    "Given these texts:\n",
    "-----\n",
    "{context}\n",
    "-----\n",
    "Please answer the following question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the PromptTemplate:\n",
    "# - `template` contains the prompt text with placeholders.\n",
    "# - `input_variables` specifies which variables will be filled in (`context` and `query`).\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"query\"],\n",
    ")\n",
    "\n",
    "# Create a question-answering (QA) chain:\n",
    "# - The chain will use the LLM to generate an answer based on the provided documents.\n",
    "# - The documents will be passed in through the `context` variable, and the question via the `query`.\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Run the QA chain:\n",
    "# - Invoke the chain with a dictionary containing the `context` (reordered documents)\n",
    "#   and the `query` (the question we're asking about the Celtics).\n",
    "response = chain.invoke({\"context\": reordered_docs, \"query\": query})\n",
    "\n",
    "# Print the response generated by the LLM based on the context and query.\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways\n",
    "* \"Lost in the Middle\" Effect: LLMs might overlook documents in the middle of a long context, so placing important ones at the extremes helps.\n",
    "* Reordering Strategy: Use tools like LongContextReorder to rearrange retrieved documents, keeping relevant info at the beginning and end.\n",
    "* Practical Benefit: This can improve the model's ability to pick up key details, enhancing performance in RAG applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
