{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks in LangChain provide a way to hook into various stages of the LLM application's execution. These allow you to monitor or modify the behavior of chains, tools, or LLMs during runtime. They are especially useful for logging, debugging, or triggering actions based on certain events in the execution process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LangChain, callbacks can be passed at runtime to capture events during the execution of chains, tools, or models. This approach simplifies the process by allowing you to pass the callback handlers once, and they will automatically be used for all nested objects involved in the execution. This eliminates the need to manually attach the handlers to each individual component, like tools or models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Passing Callbacks at Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain Unknown Chain started\n",
      "Chain ChatPromptTemplate started\n",
      "Chain ended, outputs: messages=[HumanMessage(content='What is 1 + 2?', additional_kwargs={}, response_metadata={})]\n",
      "Chat model started\n",
      "Chat model ended, response: generations=[[ChatGeneration(text='3', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='3', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 15, 'total_tokens': 16, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-20324a36-0e9e-40cd-a14b-135ab1c3e5cc-0', usage_metadata={'input_tokens': 15, 'output_tokens': 1, 'total_tokens': 16, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}))]] llm_output={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 15, 'total_tokens': 16, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None} run=None type='LLMResult'\n",
      "Chain ended, outputs: content='3' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 15, 'total_tokens': 16, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-20324a36-0e9e-40cd-a14b-135ab1c3e5cc-0' usage_metadata={'input_tokens': 15, 'output_tokens': 1, 'total_tokens': 16, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='3', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 15, 'total_tokens': 16, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-20324a36-0e9e-40cd-a14b-135ab1c3e5cc-0', usage_metadata={'input_tokens': 15, 'output_tokens': 1, 'total_tokens': 16, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Dict, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.outputs import LLMResult\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class LoggingHandler(BaseCallbackHandler):\n",
    "    # This callback handler will log when the chat model starts\n",
    "    def on_chat_model_start(\n",
    "        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs\n",
    "    ) -> None:\n",
    "        print(\"Chat model started\")\n",
    "\n",
    "    # This callback handler will log when the LLM finishes execution\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n",
    "        print(f\"Chat model ended, response: {response}\")\n",
    "\n",
    "    # This callback handler will log when a chain starts\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs\n",
    "    ) -> None:\n",
    "        # Adding a check for None to avoid the error\n",
    "        chain_name = serialized.get('name') if serialized else 'Unknown Chain'\n",
    "        print(f\"Chain {chain_name} started\")\n",
    "\n",
    "    # This callback handler will log when a chain ends\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:\n",
    "        print(f\"Chain ended, outputs: {outputs}\")\n",
    "\n",
    "\n",
    "# Instantiate the callback handler\n",
    "callbacks = [LoggingHandler()]\n",
    "\n",
    "# Set up the LLM (ChatOpenAI) and prompt template\n",
    "llm = ChatOpenAI(model=\"gpt-4\")  # Change to your desired model\n",
    "prompt = ChatPromptTemplate.from_template(\"What is 1 + {number}?\")\n",
    "\n",
    "# Combine the prompt and LLM into a chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Invoke the chain, passing the callback handlers as part of the config\n",
    "chain.invoke({\"number\": \"2\"}, config={\"callbacks\": callbacks})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to attach callbacks to a runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To attach callbacks to a Runnable in LangChain, you can use the .with_config() method, which allows you to bind runtime configuration, such as callbacks, to your Runnable. This makes it easier to reuse the callbacks across multiple invocations without passing them in each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain Unknown Chain started\n",
      "Chain ChatPromptTemplate started\n",
      "Chain ended, outputs: messages=[HumanMessage(content='What is 1 + 2?', additional_kwargs={}, response_metadata={})]\n",
      "Chat model started\n",
      "Chat model ended, response: generations=[[ChatGeneration(text='3', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='3', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 15, 'total_tokens': 16, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5076244f-c01f-4adb-b0fc-b19299add7fb-0', usage_metadata={'input_tokens': 15, 'output_tokens': 1, 'total_tokens': 16, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}))]] llm_output={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 15, 'total_tokens': 16, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None} run=None type='LLMResult'\n",
      "Chain ended, outputs: content='3' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 15, 'total_tokens': 16, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-5076244f-c01f-4adb-b0fc-b19299add7fb-0' usage_metadata={'input_tokens': 15, 'output_tokens': 1, 'total_tokens': 16, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='3', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 15, 'total_tokens': 16, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5076244f-c01f-4adb-b0fc-b19299add7fb-0', usage_metadata={'input_tokens': 15, 'output_tokens': 1, 'total_tokens': 16, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Dict, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.outputs import LLMResult\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class LoggingHandler(BaseCallbackHandler):\n",
    "    # Callback to log when the chat model starts\n",
    "    def on_chat_model_start(\n",
    "        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs\n",
    "    ) -> None:\n",
    "        print(\"Chat model started\")\n",
    "\n",
    "    # Callback to log when the LLM finishes execution\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n",
    "        print(f\"Chat model ended, response: {response}\")\n",
    "\n",
    "    # Callback to log when a chain starts\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs\n",
    "    ) -> None:\n",
    "        # Check if serialized is not None to prevent errors\n",
    "        chain_name = serialized.get('name') if serialized else 'Unknown Chain'\n",
    "        print(f\"Chain {chain_name} started\")\n",
    "\n",
    "    # Callback to log when a chain ends\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:\n",
    "        print(f\"Chain ended, outputs: {outputs}\")\n",
    "\n",
    "\n",
    "# Instantiate the callback handler\n",
    "callbacks = [LoggingHandler()]\n",
    "\n",
    "# Set up the LLM and prompt template\n",
    "llm = ChatOpenAI(model=\"gpt-4\")  # Replace with the desired model\n",
    "prompt = ChatPromptTemplate.from_template(\"What is 1 + {number}?\")\n",
    "\n",
    "# Combine the prompt and LLM into a chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Attach the callbacks using the .with_config() method\n",
    "chain_with_callbacks = chain.with_config(callbacks=callbacks)\n",
    "\n",
    "# Invoke the chain with the given input\n",
    "chain_with_callbacks.invoke({\"number\": \"2\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Points:\n",
    "1. Creating the Callback Handler: The LoggingHandler class is derived from BaseCallbackHandler and overrides various callback methods such as on_chat_model_start, on_llm_end, on_chain_start, and on_chain_end to print messages when certain events occur during execution.\n",
    "\n",
    "2. Attaching the Callbacks: The .with_config(callbacks=callbacks) method attaches the LoggingHandler to the chain. This ensures that all callbacks specified in the callbacks list will be triggered during the execution of the chain.\n",
    "\n",
    "3. Invocation of the Chain: When chain_with_callbacks.invoke({\"number\": \"2\"}) is called, the callbacks are automatically triggered during the execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Output:\n",
    "\n",
    "This will produce log messages for each stage in the process, such as:\n",
    "\n",
    "\"Chain started\"\n",
    "\n",
    "\"Chat model started\"\n",
    "\n",
    "\"Chain ended\"\n",
    "\n",
    "\"Chat model ended, response: [response data]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
