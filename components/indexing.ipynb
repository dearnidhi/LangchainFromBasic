{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "Indexing is the process of keeping your vectorstore in-sync with the underlying data source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use the LangChain indexing API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Use the Indexing API?\n",
    "The indexing API saves time and resources by:\n",
    "\n",
    "* Avoiding Duplication: It prevents the same document from being re-added.\n",
    "* Tracking Changes: It only updates changed documents, reducing unnecessary re-computation of embeddings.\n",
    "* Keeping Everything in Sync: When documents change or get deleted, the indexing API can ensure your vector store matches the latest data.\n",
    "\n",
    "# How It Works\n",
    "The indexing API uses a RecordManager to keep track of documents. It works with unique document identifiers, allowing it to manage:\n",
    "\n",
    "* Document Hashes: Ensures duplicate content isn’t added.\n",
    "* Write Timestamps: Tracks when documents were last updated.\n",
    "* Source IDs: Each document has metadata to identify its original source, like a file name.\n",
    "\n",
    "# Deletion Modes\n",
    "When documents are added or updated, you can choose how aggressively the API cleans up old versions or deleted content:\n",
    "\n",
    "* None: No automatic cleanup, just de-duplicates.\n",
    "* Incremental: Continuously deletes old versions of modified documents.\n",
    "* Full: Cleans up all old documents at the end of each indexing session.\n",
    "\n",
    "# Use Case Examples:\n",
    "\n",
    "* None: Good for first-time indexing or manual cleanup.\n",
    "* Incremental: Updates continuously and minimizes \"overlap\" of old and new data.\n",
    "* Full: Best for bulk updates when you need a clean reset for removed documents.\n",
    "\n",
    "# Requirements\n",
    "* The API works only with vector stores that support adding documents with IDs and deleting them by ID, like Chroma, Pinecone, Elasticsearch, etc. Avoid using the API with stores that were filled with other methods since the record manager might not recognize previous entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: OpenAI provides robust APIs for embedding and language generation.\n",
      "Result 2: FAISS is an efficient library for similarity search and clustering.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Step 1: Initialize the embedding model (OpenAI in this example)\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Sample documents to index\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain makes it easy to work with LLMs and chains of calls.\"),\n",
    "    Document(page_content=\"OpenAI provides robust APIs for embedding and language generation.\"),\n",
    "    Document(page_content=\"FAISS is an efficient library for similarity search and clustering.\"),\n",
    "]\n",
    "\n",
    "# Step 2: Create FAISS index from documents\n",
    "# We’ll embed and index the documents using the embedding model.\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# Step 3: Perform a similarity search\n",
    "query = \"How to use OpenAI embeddings?\"\n",
    "query_embedding = embedding_model.embed_query(query)\n",
    "\n",
    "# Search for top 2 most similar documents\n",
    "results = vectorstore.similarity_search_by_vector(query_embedding, k=2)\n",
    "\n",
    "# Step 4: Display results\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"Result {i}: {result.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Loader\n",
    "You can also create a custom loader to load and split documents, making sure to set the source metadata for each document.\n",
    "\n",
    "# In Summary\n",
    "LangChain’s Indexing API is useful for:\n",
    "\n",
    "* Managing document updates and deletions\n",
    "* Avoiding redundant computations\n",
    "* Keeping vector store data synchronized with document sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
